---
title: "08-PML-Course-project"
author: "Sebastian"
date: "8/27/2019"
output: 
  html_document: 
    keep_md: yes
---

## Synopsis

Human Activity Recognition Machine Learning Prediction Project

Human Activity Recognition - HAR - has emerged as a key research area in the last years and is gaining increasing attention by the pervasive computing research community (see picture below, that illustrates the increasing number of publications in HAR with wearable accelerometers), especially for the development of context-aware systems. There are many potential applications for HAR, like: elderly monitoring, life log systems for monitoring energy expenditure and for supporting weight-loss programs, and digital assistants for weight lifting exercises. In this classification project different algos are used to predict the correct 'classe' factor variable value in this course assignment. The winning model fit is very accurate, so that we are confident to be able to predict all 20 cases correctly.

## Data & io

This dataset with 5 classes (sitting-down, standing-up, standing, walking, and sitting) was collected on 8 hours of activities of 4 healthy subjects. 

The data for this project come from this source: http://web.archive.org/web/20161224072740/http:/groupware.les.inf.puc-rio.br/har. If you use the document you create for this class for any purpose please cite them as they have been very generous in allowing their data to be used for this kind of assignment.
```{r cache=TRUE}
options(scipen=999)
if(!exists('training')) {
    training <- read.table("pml-training.csv", comment.char = "#", 
                  header = TRUE, sep = ",", na.strings = c("NA","NaN","","#DIV/0!"))
}

if(!exists('testing')) {
    testing <- read.table("pml-testing.csv", comment.char = "#", 
                  header = TRUE, sep = ",", na.strings = c("NA","NaN","","#DIV/0!"))
}
```

## Data Wrangling & Munching

```{r}
training.lean <- training

# remove na from training factor variables
i <- sapply(training.lean, is.factor) # Identify all factor variables in your data
training.lean[i] <- lapply(training.lean[i], as.character) # Convert factors to character variables
training.lean[is.na(training.lean)] <- 0 # Replace NA with 0
training.lean[i] <- lapply(training.lean[i], as.factor) # Convert character columns back to factors

testing.lean <- testing

# remove na from testing factor variables
i <- sapply(testing.lean, is.factor) # Identify all factor variables in your data
testing.lean[i] <- lapply(testing.lean[i], as.character) # Convert factors to character variables
testing.lean[is.na(testing.lean)] <- 0 # Replace NA with 0
testing.lean[i] <- lapply(testing.lean[i], as.factor) # Convert character columns back to factors

# remove predictors with low variation as more than 19K rows are zero
zero <- function(x) sum(x == 0) > 19000
num <- c(t((numcolwise(zero)(training.lean))))
training.lean <- training.lean[,!num]
testing.lean <- testing.lean[,!num]

#remove first 6 columns as these are soley circumstantial and not able to predict the classe outcome
training.lean <- training.lean[,7:60]
testing.lean <- testing.lean[,7:60]

#sapply(training.lean, function(x) sum(is.na(x)))
```

## Data Partitioning

```{r}
# create a list of 80% of the rows in the original dataset we can use for training
validation_index <- createDataPartition(training.lean$classe, p=0.80, list=FALSE)
# select 20% of the data for validation
data.val <- training.lean[-validation_index,]
# use the remaining 80% of data to training and testing the models
data.train <- training.lean[validation_index,]

# training predictors and outcome separated
x <- data.train[, names(data.train) != "classe"]
y <- data.train[,c('classe')]
```

## EDA

it is difficult to plot with so many variables, we waive to plot anything and instead get a feeling for the frequency of each classe factor.

```{r}
levels(data.train$classe)
table(data.train$classe )
table(data.val$classe )

```

## Validation concept

We go for 10-fold cross-validation to estimate the accuracy by resampling 10 times.
This will divide our data set into 10 parts, train in 9 and test on 1. 
We waive to have repeated cross validation runs in order to reduce running time.
The metric will always be Accuracy.


```{r}
# Run algorithms using 10-fold cross validation
crtl <- trainControl(method="cv", number=10)
metric <- "Accuracy"
```


## Algorithm choice

We pick five different Algorithms from the PML studies for this classification task from linear to  complex nonlinear types.

```{r cache=TRUE, eval=FALSE}
library(caret)

set.seed(88)
# Linear discriminant Analysis
fit.lda <- train(classe~., data=data.train, method="lda", metric=metric, trControl=crtl)

set.seed(88)
# CART
fit.cart <- train(classe~., data=data.train, method="rpart", metric=metric, trControl=crtl)

set.seed(88)
# boost gbm
fit.gbm <- train(classe~., data=data.train, method="gbm", metric=metric, trControl=crtl, verbose=FALSE)

set.seed(88)
# SVM
fit.svm <- train(classe~., data=data.train, method="svmRadial", metric=metric, trControl=crtl)

set.seed(88)
# Random Forest
fit.rf <- train(classe~., data=data.train, method="rf", metric=metric, trControl=crtl)

```

```{r}

# summarize accuracy of models
results <- resamples(list(lda=fit.lda, cart=fit.cart, gbm=fit.gbm, svm=fit.svm, rf=fit.rf))
summary(results)
```

The in-model-building 10 fold cross-validation process resulted in a mean out-of-sample error of `r (1 - summary(results)$values[,9][4]) * 100`%.

```{r}
# compare accuracy of models
dotplot(results)

# summarize Best Model
print(fit.rf)
```

## Additional Validation

use the large validation data set with its 3923 observations to check the models accuracy.

```{r}
predictions <- predict(fit.rf, data.val)
performance <- confusionMatrix(predictions, data.val$classe)
print(performance)
```


Very good! the RandomForest model achieves an `r performance$overall[1]*100`% accuracy on this unseen data set, which is what he hoped for. We can conclude that this final model is not likely be overfitted and may perform well also on the official test cases.
Generally we expect an generalization error / out of sample error greater than in the sample. In this case it is `r (1-performance$overall[1])*100`%.

## Diagnosis

We check what predictors are of highest importance in this algorithm and if this is plausible. Also we double check again if we really all these variables are suitable and good predictors.

```{r}
varimportance <- varImp(fit.rf)
varimportance
```

## Predictions

Now we use our RandomForest model with the assigned official test data set and its 20 observations to predict the classe for each observation for course submission.

We account for an generalization error / out of sample error greater than seen with training and validation data set.

```{r}
# predict 20 test case with the best model
pred.rf.testing.lean <- predict(fit.rf, testing.lean)

table(pred.rf.testing.lean)

print(pred.rf.testing.lean)

```

## Reproducability

This R markdown files is complete and runnable.
However the rendering did only succeed from the console with

`library("knitr")`

`rmarkdown::render("08-PML-Course-project.Rmd")`
